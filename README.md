## Распределенные данные от Алекса Петрова

Изначально я планировал выкладывать конспекты по каждой главе у себя в tg канале, но вскоре понял, что телега - не лучшее место для конспектов по книгам, поэтому я перенес его сюда. 
Сказать, что это конспект в привычном понимании этого слова было бы неправильным, я бы назвал это смесью конспекта с щепотками собственного мнения под соусом странной манеры письма. 
Цель написания таких конспектов заключается в том, чтобы более глубоко разбирать сложные книги, а также иметь возможность освежить свою память спустя некоторое время. Я надеюсь писать такие конспекты по каждой технической книге, за чтение которой я взялся, ибо это даст мне ощутимый буст в перспективе. 

## Table of contents
- [Глава 2. Введение в B-деревья](#chapter2)
- [Глава 3. Форматы файлов](#chapter3)
- [Глава 4. Реализация B-деревьев в БД](#chapter4)
- [Глава 5. Обработка транзакций и восстановление](#chapter5)
- [Глава 6. Варианты B-деревьев](#chapter6)
- [Глава 7. Журналированное хранилище](#chapter7)
- [Глава 8. Часть I. Заключение](#chapter8)
- [Bonus. Database Internals CMU course](#cmu-course)

<a name="chapter2"></a> 
### Глава 2. Введение в B-деревья
Эта глава посвящена теме, с которой я познакомился еще будучи студентом колледжа, и к которой я возвращаюсь по сей день - это B-Tree. Да, та самая структура данных, которую умные люди в области создания этих сложных СХД взяли за основу построения реляционных баз данных типа PostgreSQL или MySQL. Для начала хорошо бы понять, а почему вообще упарываться с такой сложной структурой данных, когда можно использовать стандартное двоичное дерево. Ну да, может быть проблема с балансированием таких деревьев, но есть же сбалансированные варианты типа AVL, Red-Black Tree и т.д. Секрет кроется в возможностях сегодняшних жестких дисков и SSD. Если не знаете, как устроен диск, то вкратце скажу, что состоит он из движущейся головки, которая считывает информацию с пластин. Причем, головка читает данные последовательно и читает она эти данные секторами размером от 512Б до 4КБ - не побайтово. Аналогия с механизмом выделения памяти в ОС, где память отдается приложению в виде страниц фиксированного размера. Т.е чтобы прочесть какие-либо данные, должно произойти позиционирование головки, что является достаточно затратной операцией, а далее наступает процесс последовательного чтения, что происходит достаточно быстро. SSD же устроены несколько иначе: есть ячейки памяти, которые объединяются в строки (обычно от 32 до 64 ячеек на строку), строки объединяются в массивы, массивы - в страницы, а страницы - в блоки. Блоки объединяются в пластины, а пластины уже образуют кристалл. Причем, размер ячейки памяти зависит от используемой технологии. Соответственно, размер одной страницы может варьироваться от 2 до 16КБ. В SSD минимальной единицей, которую можно прочитать или записать, является страница. Но стирание данных осуществляется поблочно, этим занимается отдельный компонент внутри SSD, называемый FTL (Flash Translation Layer). Но самое главное, что в отличие от тех же жестких дисков, с произвольным вводом-выводом у SSD все гораздо лучше. На этом этапе вы, возможно, задаете себе вопрос: "Ну и зачем мне это щас нужно было", ну а я отвечу, что благодаря этой инфе теперь можно подобрать структуру данных, которая обладает следующими свойствами: 

A. У структуры данных высокая степень ветвления (т.е возможность для одного узла иметь большое кол-во потомков), таким образом можно было бы обеспечить лучшую локальность соседних ключей (держать соседние ключи ближе друг к другу).\
Б. Структура данных не предусматривает наличие большой высоты, что сокращает количество операций дискового поиска во время обхода. 

Что ж, ну и умные люди по имени Байер и МкКрейт в 1972 и создали такую структуру данных как B-Tree. Вообще если хотите на интуитивном уровне понять, как она работает, то лучше всего посмотреть видео. Я читал достаточно много литературы (сюда входят и статьи), где описывают принцип ее работы, но ничего простого не нашел. А лучше всего посмотреть видео, а потом почитать статьи. Ну и нет ничего лучше идеи реализовать это на каком-нибудь ЯП. 
Для начала посоветую вот это [видео](https://www.youtube.com/watch?v=K1a2Bk8NrYQ).  
Есть еще вот такой [сайт](https://www.cs.usfca.edu/~galles/visualization/BTree.html), где можно интерактивно изучить принцип работы разных структур данных, в том числе и B-Tree. 

<a name="chapter3"></a> 
### Глава 3. Форматы файлов
Новая глава - новый пост. Сегодня поговорю о способе организации данных на диске, а именно о том, как представить узлы B-Tree на каком-либо дисковом носителе. Во-первых, нужно сказать, что для простоты разработчики СУБД решили выделять на один узел дерева одну страницу. Например, в оригинальной статье с описанием B-Tree описывался простейший способ организации этих страниц для записи данных, где это выглядит как-то вот так

| p0 | k1 | v1 | p1 | k2 | v2 | p2| .... |kn| |vn| |pn| 
где:

p - указатель на дочерние страницы
k - ключ
v - значение

Но такой подход обладает недостатками. 
1. Если новая запись добавляется не справа, то возникает необходимость перемешивать элементы
2. Данный способ подходит только для данных фиксированного размера
3. Если хранить в таком виде данные переменного размера, возникает проблема с фрагментацией, т.е когда у нас есть много свободных кусков разного размера, появившихся в результате освобождения ненужных ячеек. При такой организации достаточно сложно дефрагментировать память. 

Для устранения этих проблем была придумана структура слотированных страниц (slotted page). Выглядит она как-то так 

<img width="755" alt="pic" src="https://github.com/user-attachments/assets/1bbbce6c-12c1-495b-a415-41b4d33460c7">

В данной схеме у нас есть: 

Заголовок, содержащий в себе метаинформацию, типа id страницы, количество свободного места, указатели на начало свободного и конец свободной памяти, различные флаги, тип узла (ROOT, INTERIOR, LEAF).

Указатели, указывающие на ячейки. Причем, указатели отсортированы для возможности двоичного поиска

Ну и сами ячейки, хранящие пары ключ/запись. 

Такая структура позволяет достаточно гибко как вставлять, так и удалять данные. В случае удаления указатель на ячейку может быть помечен как "not used", ну а на странице может быть выставлен флаг CAN_COMPACT. Через какое-то время гарбедж коллектор просто скомпактит страницу, освободив место для будущих вставок. 

Вот тут более подробно рассказывается про слотированные страницы 
https://siemens.blog/posts/database-page-layout/
Еще и код есть, что вообще неплохо. 

<a name="chapter4"></a> 
### Глава 4. Реализация B-деревьев в БД
Продолжаем говорить о B-деревьях. Это уже 3 глава о них, да. Вот такие они сложные и важные. Во-первых, надо повторить, что каждому узлу в дереве соответствует одна страница. Одна такая страница содержит заголовок, отсортированные смещения (или указатели на ячейки) и сами ячейки, содержащие пары ключ/запись. Каждый файл, содержащий данные, начинается с магического числа (Magic number). Обычно это набор каких-то байтов, которые помогают понять, с каким файлом мы имеем дело.
 
**Линки между узлами** \
Представим дерево с одним корневым узлом и 2 дочерними узлами child1 и child2. Путь от одного дочернего узла к другому дочернему узлу содержит посещение корневого узла. Некоторые реализации избегают последнего путем создания линков между одноуровневыми узлами. Учитывая, что все ключи на уровне отсортированы, это достаточно хороший подход, экономящий время поиска. Минусом такого подхода является экстра оверхед при апдейте линков при разделении и слиянии. 

**Крайние правые указатели и высокие узлы ключа** \
Дисклеймер: здесь я буду описывать подходы, значение которых я пока совсем не догоняю, но для чего-то разработчики СУБД их применяют. 
Подход крайнего правого указателя подразумевает хранение крайнего правого указателя в узле в заголовке страницы. Этот указатель не связан с каким-либо ключом. 
Подход с высоким узлом ключа подразумевает добавление еще одного ключа, который задает верхнюю границу в узле. Данный ключ является парой для крайнего правого указателя. 
Оба подхода используются в СУБД sqlite и PostgreSQL соответственно. 

**Страницы переполнения** \
Об этом я должен был, возможно, рассказать в предыдущем посте, но я скинул ссылку на статью, которая содержала в себе описание этого механизма. Лучше всего этот процесс можно понять, представив себе ситуацию. Представим, что в один из столбцов базы вы пишете целое полотно текста, полотно это настолько большое, что место в странице заканчивается уже после после половины записанного текста. В таком случае создается доп. страница переполнения, на которую ссылается страница с началом вашего полотна. Таких страниц может быть несколько в зависимости от ваших записей. Все они представляют собой связанный список. 

**Распространение операций разделения и слияния** \
Как вы уже знаете, узлы в B-Tree могут делиться и склеиваться. В таком случае нам нужно распространить эти операции в рамках всего дерева, т.е переназначить указатели. Для нахождения целевого узла нам необходимо проделать путь до корня дерева, а затем спуститься обратно рекурсивно вниз. 

**Навигационная цепочка** \
Вместо того чтобы хранить и поддерживать указатели на родительские узлы, можно хранить путь к узлу в какой-нибудь структуре данных, ну и в случае каскадного разделения или слияния можно пройти 

**Перебалансировка** \
Чтобы улучшить распределение нагрузки, некоторые реализации производят операцию перебалансировки, т.е переносят ключи и более загруженных узлов в менее загруженные узлы. 

**Добавление только справа** \
Многие СУБД используют в качестве первичного ключа авто-инкрементирующиеся значения. Это открывает возможность для оптимизации. Если вставляемый ключ больше первого ключа в крайнем правом узле, то мы можем положить ключ в закешированную крайнюю правую страницу, исключая необходимость обхода дерева в поисках нужного места. 

**Сжатие** \
Еще один необходимый элемент для эффективного хранения данных. С ростом количества данных растут и требования к объему носителей. Чтобы замедлить этот процесс, разработчики прибегают к сжатию. Алгоритмов сжатия достаточно много, но при выборе нам нужно обращать внимание на такие параметры как степень сжатия, производительность и издержки памяти.  Например, Clickhouse по дефолту использует zstd. После выбора алгоритма встает вопрос о том, что сжимать. Сжимать целый файл - как-то не комильфо. Это означает, что при каждом доступе к какой-либо из страниц нам придется вернуть весь файл в начальное состояние, достать нужную страницу и т.д. Поэтому появилась идея сжимать данные постранично. 

**Очистка и обслуживание** \
Записи в базе периодически удаляются. Конечно же, СУБД не удаляет их сразу, так как это очень дорого. Вместо этого данные помечаются нулями. Точнее, на эти данные нет никаких указателей, соответственно их можно считать мусором. Для этого в ряде СУБД существует фоновый процесс чистки, своего рода гарбэдж коллектор, который эту работу и делает. В качестве доп. чтения привожу 2 полезные статьи:
https://www.lucavall.in/blog/how-databases-store-and-retrieve-data-with-b-trees
https://fly.io/blog/sqlite-internals-btree

P.S 
Я все больше склоняюсь к тому, что на чтение одной главы этой книги нужно уделять неделю, ибо полностью понять все за сутки без наличия каких-либо знаний по проектированию СХД достаточно проблематично.

<a name="chapter5"></a> 
### Глава 5. Обработка транзакций и восстановление

Да, сегодня поговорим про транзакции, но не те транзакции, про которые я говорил в постах по битку, а про транзакции в контексте СУБД. Наряду с этим термином часто упоминают еще ACID или Atomicity, Consistency, Isolation и Durability. По порядку: 


***Атомарность***. Другими словами неделимость. Это свойство транзакций, при котором не допускается частичное выполнение команд в ее пределах. Если еще проще, то транзакция либо выполняется полностью, либо вообще не выполняется. Тут можно провести параллель с атомиками в языках программирования.

***Согласованность***. Вообще этот термин очень перегружен в среде CS, и, соответственно, может означать разное в зависимости от контекста. В данном случае означает, что база должна переходить из одного допустимого состояния в другое допустимое состояние, сохраняя все инварианты базы (включая различные ограничения, ссылочную целостность и т.д)

***Изоляция***. В современных бдшках несколько транзакций могут выполняться одновременно, ну и хорошо, если они не будут мешать друг другу. Изоляция определяет, когда изменения, сделанные одной транзакцией, должны стать видимыми другим транзакциям. Для этого люди вывели несколько уровней изоляции, о которых мы поговорим ниже. А пока только скажу, что чем сильнее уровень изоляции, тем менее производительна бдшка. 

***Долговечность***. После коммита все транзакции должны храниться на персистентном носителе, i.е диске и сохраняться в случае перебоев, отказов и т.д

Конечно же, для поддержания этих свойств в СУБД есть отдельные компоненты. Они перечислены ниже: 

***Диспетчер транзакций***. Отвечает за планирование и координацию транзакций. 

***Диспетчер блокировок***. Контролирует доступ к данным, не допуская при этом параллельных обращений, способных нарушить целостность данных. 

***Кэш страниц***. Кеширует страницы с персистентного носителя. Все операции производятся с страницами в кеше.
 
***Диспетчер журналов***. Хранит историю операций (записи журнала), примененных к кэшированным страницам, но еще не синхронизированных с персистентным носителем,
чтобы изменения не были потеряны в случае сбоя. 

#### Организация буферизации данных
Обычно в СУБД используется двухуровневая система буферизации данных. Обычно СУБД работает со страницами в кэше. Но нужна гарантия, что другие процессы не будут менять страницы на диске. Когда страница становится грязной, т.е она подверглась изменению находясь в ОЗУ, она выгружается на диск. Подобная система присутствует в ОС. Они используют незанятые сегменты памяти, чтобы прозрачно кешировать содержимое диска для повышения производительности системных вызовов ввода-вывода.   

**ОБХОД СТРАНИЧНОГО КЭША ЯДРА** 

Многие СУБД открывают файлы с помощью флага O_DIRECT. Этот флаг позволяет системным вызовам ввода-вывода обходить кэш страниц ядра, обращаясь к диску напрямую и используя управление буфером конкретной базы данных. К этому порой неодобрительно
относятся разработчики операционных систем. Линус Торвальдс не одобряет (https://databass.dev/links/32) использование флага O_DIRECT по той причине, что такой подход не асинхронен и не предусматривает упреждающее чтение или другие средства для информирования ядра о схемах доступа. Однако до тех пор,
пока операционные системы не начнут предлагать лучшие механизмы, флаг O_DIRECT по-прежнему будет полезен.
Мы можем получить некоторый контроль над процессом вытеснения страниц из кэша операционной системы, используя функцию fadvise (https://databass.dev/links/33), но такой подход позволяет лишь попросить ядро учесть наше мнение и не гарантирует, что оно так
и сделает. Чтобы отказаться от системных вызовов при вводе-выводе, мы можем использовать отображение в память, но тогда мы теряем контроль над кэшированием. Более подробно про проблему отображения в память можно узнать вот здесь https://db.cs.cmu.edu/papers/2022/cidr2022-p13-crotty.pdf


#### Вытеснение из кэша 

Нужно стремиться к тому, чтобы кэш всегда был заполнен полностью. Но все страницы в кэш мы положить не можем, соответственно нам нужно удалять из кэша страницы, которые "не нужны". Беру последнее в кавычки, ибо значение этого выражения зависит от алгоритма вытеснения. В случае грязных страниц СУБД также обязан выгрузить их на диск перед удалением из кэша. Но выгружать грязные страницы при каждой операции вытеснения дорого, поэтому этим может заниматься отдельный процесс. Пример такого фонового процесса представлен в PostgreSQL. Подробнее он описан вот здесь https://www.interdb.jp/pg/pgsql08/06.html

Также важно сохранять свойство долговечности. Для этого должен существовать фоновый процесс, который будет выгружать изменения на персистентный носитель. Причем, для этого будут существовать чекпоинты. В PostgreSQL такой процесс называется checkpointer. 

#### Политики вытеснения
По политикам вытеснения напишу только, что существует всеми любимый LRU, LFU, FIFO, а также придуманный дядей Таненбаумом CLOCK. 
TODO: написать про алгоритм работы TinyLFU. 

#### Восстановление

Мы все должны признать, что мир распределенных систем нестабилен. Соответственно, нам нужно как-то уметь восстанавливать состояние СУБД до сбоя. Для этого люди придумали WAL (Write-Ahead-Logging) - append-only структура, которая выполняет следующие функции

1. Позволяет кэшу страниц буферизировать обновления размещенных на диске страниц с обеспечением долговечности в общем контексте СУБД.
1. Производит персистентное сохранение всех операций на диске до тех пор, пока кэшированные копии страниц, затронутых этими операциями, не будут синхронизированы с копиями на диске. Каждая операция, которая изменяет состояние базы данных, должна быть записана в журнале на диске до изменения содержимого соответствующих страниц.
1. Позволяет в случае сбоя восстановить из журнала операций потерянные изменения, которые были произведены в памяти


Кроме того WAL также играет немаловажную роль в обработке транзакций. Даже если данные не были зафиксированы, они будут доступны даже после сбоя. 

#### Семантика журнала 
Журнал упреждающей записи состоит из записей. Каждая запись имеет уникальный, монотонно увеличивающийся порядковый номер журнала (log sequence number, LSN). Обычно этот номер представляет собой внутренний счетчик или временную метку. Поскольку записи журнала не всегда занимают целый дисковый блок, их содержимое кэшируется в буфере журнала и выгружается на диск в ходе принудительной выгрузки. Принудительная выгрузка выполняется по мере заполнения буферов журнала и может запрашиваться диспетчером транзакций или кэшем страниц. Все записи
журнала должны выгружаться на диск согласно порядковому номеру журнала LSN.

#### Использование операций и журнала данных 
Тут автор начал с описания техники shadow paging, ну а потом перепрыгнул на описание логического и физического журнала. Не совсем понял этот момент, но опишу только разницу между логическим и физическим журналом. Логический журнал представляет собой историю операций, которые были произведены СУБД. Например, там могут быть записи типа *вставка записи данных X для ключа Y*. Физический журнал, в свою очередь, хранит историю изменений на физическом носителе, например, изменения в какой-либо странице и т.д. Современные СУБД комбинируют оба подхода. 

#### Политики кражи и принуждения
Для определения подходящего времени выгрузки на диск произведенных в памяти изменений СУБД используют политики "кражи"/"без кражи" и "принуждения"/"без принуждения".
Если коротко, то политика "кражи" позволяет СУБД выгружать измененные транзакцией страницы на диск еще до фиксации для того чтобы освободить место в кеше страниц, тогда как политика "без кражи" не позволяет делать это. Политика "принуждения" требует, чтобы все измененные транзакцией страницы были выгружены на диск еще до фиксации тогда как политика "без принуждения" позволяет транзакции зафиксироваться даже если измененные страницы не были выгружены на диск. 

#### ARIES 
Одним из примеров алгоритмов для восстановления после сбоя в СУБД служит алгоритм ARIES. Он принадлежит к группе алгоритмов с типом кражи без принуждения (steal/no force). Для своей работы он полагается на WAL. Согласно этому алгоритму в ходе перезапуска после сбоя восстановление происходит в 3 этапа

1. На этапе анализа выявляются "грязные" страницы в кэше страниц и транзакции, выполнявшиеся во время сбоя. Информация о "грязных" страницах используется для определения начальной точки для этапа повтора. Список незавершенных транзакций используется на этапе отмены для их отката
2. Этап повтора вновь применяет всю историю вплоть до момента сбоя и восстанавливает прежнее состояние БД. Этот этап выполняется для незавершенных транзакций, а также для транзакций, которые были зафиксированы без выгрузки их содержимого на персистентный носитель.
3. Этап отмены откатывает все незавершенные транзакции и восстанавливает последнее согласованное состояние БД. Все операции откатываются в обратном хронологическом порядке. На случай повторного сбоя БД во время восстановления все операции, отменяющие транзакции, также регистрируются в журнале во избежание их повторного выполнения.

Алгоритм полагается на физический лог для повтора и логический лог для отмены транзакций. 

#### Управление параллелизмом
Управление параллелизмом - это набор методов, обеспечивающих взаимодействие параллельно выполняющихся транзакций. Эти методы можно разделить на следующие категории: 

***Оптимистичное управление параллелизмом (Optimistic concurrency control, OCC)***
Позволяет транзакциям производить параллельное чтение и запись данных, ну а перед фиксацией транзакции проверяются на возможные конфликты. В случае возникновения таких конфликтов одна из транзакций откатывается

***Управление параллелизмом с несколькими версиями (multiversion concurrency control, MVCC)***
Оперирует версиями данных. Таким образом, каждая запись имеет временную метку (или версию). Этот подход может быть реализован с помощью методов проверки, позволяющих выиграть только одной из обновляющихся или фиксирующихся транзакций, а также с помощью методов без блокировки, таких как сортировка по временным меткам, или же методов на основе блокировки, таких как двухфазная блокировка (2PL). Используется в PostgreSQL btw

***Пессимистичное управление параллелизмом (pessimistic concurrency control, PCC)***
Существуют как блокирующие, так и неблокирующие консервативные методы управления паралеллизмом. Методы на основе блокировок требуют, чтобы транзакции не получали доступ к данным, если эти на этих данных уже стоит блокировка. Неблокирующие методы оперируют списками чтения и записи и ограничивают выполнение в зависимости от планирования незавершенных транзакций. Блокирующие методы могут приводить к дедлокам. 


**Сериализуемость (Serializable)** 
Под сериализуемостью в области СУБД понимают свойство, при котором параллельное выполнение операций можно свести к последовательному. Сериализуемость также является самым сильным уровнем изоляции (о них поговорим ниже).  

***Уровни изоляции***
Наверное, самый любимый вопрос интервьюверов в секции БД. Ниже разберем виды аномалий, уровни изоляций, а в конце я приведу очень наглядную таблицу, где изображено наличие аномалий в каком-либо из уровней. Начнем с аномалии чтения. 

***Грязное чтение (Dirty read)*** - аномалия, при которой транзакции могут читать незафиксированные данные 

***Неповторяемое чтение (Nonrepeatable read)*** - аномалия, при которой транзакция запрашивает одну и ту же строку дважды, и при этом получает разные данные

***Фантомное чтение (Phantom read)*** - То же самое, что неповторяемое чтение, но тут речь идет уже о целом диапазоне данных

Наряду с аномалиями чтения существуют еще и аномалии записи

***Потерянное обновление (Lost update)*** - аномалия, при которой обе транзакции меняют значение какой-либо записи, и при этом одна из записей не учитывается (или теряется)

***Грязная запись (Dirty write)*** - ситуация, при которой транзакция осуществляет "грязное" чтение, изменяет целевую запись и фиксирует это изменение. 

***Искажение записи (write skew)*** - происходит, когда транзакции соблюдают требуемые инварианты, но комбинация их действий не удовлятворяет общему результату. В книге приводится следующий пример 
```
Например, транзакции T1 и T2 изменяют значения двух счетов A1 и A2.
Вначале A1 содержит 100$, а A2 — 150$. Значение счета может быть отрицательным,
если сумма двух счетов неотрицательна: А1 + А2 >= 0. Транзакция T1 пытается снять
200$ со счета A1, а транзакция T2 — со счета A2. Так как в момент начала этих транзак-
ций A1 + A2 = 250$, то в сумме доступно 250$. Обе транзакции предполагают, что они
сохраняют инвариант и могут быть зафиксированы. Однако после фиксации счет A1
будет содержать 100$, а счет A2 — –50$, что явно нарушает требование о том, чтобы
сумма счетов всегда была положительной
```
Все понятно до момента с остатком на первом счете 100$, а на втором - -50$. 

**Уровни изоляции**

***Read uncommitted***. Является самым слабым уровнем изоляции, который допускает "грязное" чтение.
***Read committed***. Посильнее read uncommitted, так как не допускает "грязного" чтения, но допускает аномалию неповторяемого чтения. 
***Repeatable read***. Решает проблему неповторяемого чтения у read committed. 
***Serializable***. Как уже было сказано, является самым сильным уровнем изоляции. Если дядюшка Бен был говорил про сериализуемость, то фраза звучала бы что-то типа "With great power comes great perfomance impact". Да, работая в таком уровне не ожидайте хорошей производительности от СУБД. 

Ну а теперь приведу обещанную красивую таблицу из книги

<img width="787" alt="isolation-anomaly-pic.jpg" src="https://github.com/user-attachments/assets/20cebfb7-4338-48ea-845e-edbe77774378">


Дальше в книге идет описание методов контроля управления параллелизмом, как СУБД решают проблему дедлоков, также уделяется место видам блокировок (обычные блокировки, защелки (latches), блокировки чтения-записи (аля rw mutex)) и даже говорится про B-link деревья. Это я решил пока не писать, дабы уменьшить размер конспекта. Возможно, я сделаю это когда-нибудь в будущем, но это не точно. 

<a name="chapter6"></a>
### Глава 6. Варианты B-деревьев
TODO: вернуться к этой главе, на данный момент она вообще doesn't make sense с моими текущими знаниями в области СУБД-строения. 

<a name="chapter7"></a>
### Глава 7. Журналированное хранилище
В предыдущих главах автор достаточно подробно описал B-Tree. В этой главе рассматривается несколько иная структура дерева, называемая LSM (Log Structured Merge) Tree. Такие деревья все чаще и чаще всплывают в мире проектирования СУБД. В неизменяемых LSM-деревьях используют хранилище, доступное только для добавления, и согласование слиянием, а в B-деревьях размещают записи данных на диске и обновляют страницы по их первоначальным смещениям в файле. 
Структуры хранения с обновлением на месте оптимизированы для повышения производительности чтения: запись может быть возвращена клиенту сразу после нахождения ее данных на диске. За это приходится платить снижением производительности записи: чтобы обновить запись данных на месте, сначала нужно найти ее на диске. С другой стороны, хранилище, доступное только для добавления, оптимизировано для повышения производительности записи. Операциям записи не нужно искать записи на диске, чтобы перезаписать их. Однако за это приходится платить снижением производительности операций чтения, которые должны извлекать несколько версий записей данных и согласовывать их.

#### LSM-деревья
Говоря о B-деревьях, мы пришли к выводу, что показатели расхода пространства и увеличения объема записи можно улучшить с помощью буферизации. Как правило, существуют два способа применения буферизации в различных структурах хранения: отложить распространение записи на страницы, находящиеся на диске, и сделать операции записи последовательными. В одной из самых популярных разновидностей дисковых неизменяемых структур хранения, LSM-деревьях, для обеспечения последовательной записи используются
буферизация и хранилище, доступное только для добавления. LSM-дерево — это вариант дисковой структуры, подобной B-дереву, где узлы полностью заняты и оптимизированы для последовательного доступа к диску

Слово «слияние» (merge) в LSM-деревьях указывает на то, что из-за их неизменяемости содержимое дерева объединяется с использованием подхода, аналогичного сортировке слияния. Этот процесс производится во время обслуживания для высвобождения места, занятого избыточными копиями, и во время чтения перед возвращением содержимого пользователю. 

**Структура LSM дерева**\
LSM-деревья состоят из малых компонентов, расположенных в памяти, и больших компонентов, расположенных на диске. Чтобы записать неизменяемое содержимое файла на диск, необходимо сначала буферизировать его в памяти и отсортировать содержимое. Находящийся в памяти компонент (часто называемый резидентной таблицей, memtable), является изменяемым: он буферизует записи данных и служит в качестве цели для операций чтения и записи. Содержимое таблицы в памяти сохраняется на диске, когда его объем достигает настраиваемого порога. Для обновления резидентной таблицы не требуется доступ к диску, и оно не приводит к затратам на ввод-вывод. Отдельный файл журнала упреждающей записи необходим для гарантии долговечности за-
писей данных. Записи данных добавляются в журнал и фиксируются в памяти до подтверждения операции клиенту.
Буферизация выполняется в памяти: все операции чтения и записи применяются к резидентной таблице, которая поддерживает отсортированную структуру данных, допускающую параллельный доступ, — обычно некоторую форму резидентного отсортированного дерева или любую структуру данных, которая может обеспечить аналогичные характеристики производительности. Дисковые компоненты создаются путем выгрузки на диск буферизованного в памяти содержимого. Эти компоненты используются только для чтения: буферизованное содержимое сохраняется на персистентном носителе и файлы никогда не изменяются. Это позволяет нам мыслить в терминах простых операций: запись в резидентную таблицу, чтение с диска и из резидентных таблиц, слияние и удаление файлов.

**Двухкомпонентное LSM дерево**\
Мы различаем двух- и многокомпонентные LSM-деревья. Двухкомпонентные LSM-деревья имеют только один дисковый компонент, состоящий из нескольких неизменяемых сегментов. Дисковый компонент здесь организован как B-дерево со 100%-ной заполняемостью узлов и страницами, доступными только для чтения. Содержимое резидентного дерева выгружается на диск по частям. Во время выгрузки для каждого выгружаемого из памяти поддерева мы находим соответствующее поддерево на диске и записываем в новый сегмент на диске результат объединения содержимого сегмента, находящегося в памяти, и содержимого поддерева, находящегося на диске. 

<img width="804" alt="Capture d’écran 2024-10-28 à 6 33 18 PM" src="https://github.com/user-attachments/assets/1c8a862d-2300-4460-adb5-4c3bb2df1e7f">

После выгрузки поддерева заменяемые поддеревья в памяти и на диске удаляются и заменяются результатом их слияния, который становится адресуемым из ранее существовавших разделов дискового дерева. На рис. 7.2 показан результат процесса слияния, уже записанный в новое место на диске и присоединенный к остальной части дерева. Операция слияния реализуется с помощью пошагового продвижения итераторов, производящих одновременное считывание расположенных на диске листовых узлов и содержимого резидентного дерева. Поскольку оба источника отсортированы, для получения отсортированного объединенного результата нам нужно знать лишь текущие значения обоих итераторов на каждом шаге процесса слияния.

<img width="789" alt="Capture d’écran 2024-10-28 à 6 34 41 PM" src="https://github.com/user-attachments/assets/b9149801-7f58-4ffe-a542-d9af83d1966f">

При реализации слияния и выгрузки поддеревьев необходимо обеспечить следующее:
1. С момента начала процесса выгрузки все новые записи должны помещаться
в новую резидентную таблицу.
2. Во время выгрузки поддерева и дисковое поддерево, и выгружаемое резидентное
поддерево должны оставаться доступными для чтения.
3. После выгрузки публикация объединенного содержимого и удаление оставшегося
необъединенного содержимого на диске и в памяти должны выполняться атомарно.
Несмотря на то что двухкомпонентные LSM-деревья могут быть полезны для ведения индексных файлов, на момент написания книги автору не было известно о каких-либо реализациях. Это можно объяснить характерным для этого подхода увеличением объема записи: операции слияния производятся достаточно часто, так как они запускаются при выгрузке резидентной таблицы.

**Многокомпонентные LSM-деревья**\
Давайте рассмотрим альтернативную структуру — многокомпонентные LSM-деревья, которые имеют несколько дисковых таблиц. При этом все содержимое резидентной таблицы выгружается на диск за один подход. Очень быстро становится очевидным, что после многократных выгрузок мы получим множество дисковых таблиц и с течением времени их количество будет только расти. Поскольку мы не всегда точно знаем, какие таблицы содержат необходимые записи данных, для нахождения искомых данных иногда требуется получать доступ к нескольким файлам.
Необходимость читать из нескольких источников вместо одного может привести к большим затратам. Чтобы смягчить эту проблему и свести количество таблиц к минимуму, запускается периодический процесс слияния, называемый уплотнением (compaction). Процесс уплотнения выбирает несколько таблиц, считывает и объединяет их содержимое, после чего записывает результат объединения в новый файл. Прежние таблицы удаляются одновременно с появлением новой объединенной таблицы.

<img width="781" alt="Capture d’écran 2024-10-28 à 7 04 36 PM" src="https://github.com/user-attachments/assets/7567f0c3-fe5c-4ed9-a65d-609faf6eb822">
Ниже дается краткое описание каждого компонента:
*Текущая резидентная таблица* Принимает записи и обеспечивает чтение.
*Выгружаемая резидентная таблица* Доступна для чтения.
*Цель выгрузки, находящаяся на диске* Не участвует в операциях чтения, так как ее содержимое является неполным.
*Выгруженные таблицы* Доступны для чтения сразу после удаления выгруженной резидентной таблицы.
*Таблицы в процессе уплотнения* Дисковые таблицы, подвергаемые слиянию в данный момент.
*Уплотненные таблицы* Создаются из выгруженных или других уплотненных таблиц.

**Обновления и удаления**\
В LSM-деревьях для выполнения операций вставки, обновления и удаления не требуется искать записи данных на диске. Вместо этого обновления и удаления представлены как отдельные записи, которые согласовываются во время чтения. При этом недостаточно лишь удалить записи данных из резидентной таблицы, так как другие дисковые или резидентные таблицы могут содержать записи данных с таким же ключом. Если бы мы реализовали удаление с помощью простого удаления элементов из резидентной таблицы, то наши операции удаления в конечном итоге либо не оказывали бы никакого влияния, либо заново восстанавливали бы прежние значения. 
Давайте рассмотрим пример. Выгруженная дисковая таблица содержит запись данных v1, связанную с ключом k1, а резидентная таблица содержит новое значение этой записи v2: 

<img width="406" alt="Capture d’écran 2024-10-28 à 7 16 04 PM" src="https://github.com/user-attachments/assets/007ab191-390e-4db4-aa7b-54cbba538154">

Если мы просто удалим значение v2 из резидентной таблицы и выгрузим его, мы фактически восстановим значение v1, так как оно станет единственным значением, связанным с этим ключом:

<img width="416" alt="Capture d’écran 2024-10-28 à 7 16 22 PM" src="https://github.com/user-attachments/assets/967d955f-5e37-48ba-90c1-037088757c7f">

По этой причине удаление необходимо записывать явным образом. Это можно
обес­ печить, вставляя специальную запись удаления (иногда называемую отметкой
полного удаления или сертификатом неактивности), указывающую на удаление
записи данных, связанной с определенным ключом:

<img width="590" alt="Capture d’écran 2024-10-28 à 7 16 41 PM" src="https://github.com/user-attachments/assets/c19ae691-91d6-4dbd-98a7-7d48dd3fbafa">

Процесс согласования собирает отметки полного удаления и отфильтровывает затененные значения. Иногда бывает полезно удалить последовательный диапазон ключей, а не один ключ.
Это можно сделать с помощью предикатных операций удаления, которые добавляют запись удаления согласно предикату, отсеивающему элементы в соответствии с обычными правилами сортировки записей. В процессе согласования записи данных, соответствующие предикату, пропускаются и не возвращаются клиенту.

**Итерация слиянием**\
Итерация слиянием в LSM напоминает задачку с мержем отсортированных связных списков. 
<img width="752" alt="Capture d’écran 2024-10-28 à 7 20 00 PM" src="https://github.com/user-attachments/assets/2cfec1f3-8cb5-4de3-a399-de1d9d106635">


### Чтение, запись и увеличение пространства
При хранении неизменяемых данных на диске мы сталкиваемся с тремя проблемами:

*Увеличение объема чтения*\
Возникает в результате необходимости обращения к нескольким таблицам для извлечения данных.
*Увеличение объема записи*\
Вызывается последовательными операциями перезаписи в процессе уплотнения.
*Увеличение пространства*
Возникает в результате хранения нескольких записей, связанных с одним и тем же ключом.

**Гипотеза RUM**\
Гипотеза RUM гласит, что снижение затрат по двум параметрам неизбежно приводит к изменению в худшую сторону третьего и что оптимизация может быть выполнена только за счет одного из трех параметров. Мы можем сравнить различные подсистемы хранения данных с точки зрения этих трех параметров и понять, какие из них оптимизированы в каждом случае и какие потенциальные компромиссы это подразумевает.

### Подробнее о реализации

**Сортированные таблицы строк SSTable**\
Дисковые таблицы обычно реализуются с помощью отсортированных строковых таблиц (SSTable). Как следует из названия, записи данных в этих таблицах сортируются и располагаются согласно порядку ключей. Обычно они состоят из двух компонентов: индексных файлов и файлов данных. Индексные файлы реализуются с помощью некоторой структуры, обеспечивающей логарифмический поиск, такой как B-дерево, или структуры с постоянным временем поиска, такой как хэш-таблица. Поскольку файлы данных содержат записи согласно порядку ключей, использование хэш-таблиц для индексирования не помешает нам реализовать сканирование диапазона, поскольку доступ к хэш-таблице осуществляется только для нахождения первого ключа в диапазоне, а сам диапазон можно считывать из файла данных последовательно — пока выполняется предикат диапазона. Индексный компонент содержит ключи и записи данных (смещения в файле данных, указывающее место размещения реальных записей данных). Компонент данных состоит из сцепленных пар «ключ–значение». Устройство ячеек и форматы записей данных, которые мы обсуждали в главе 3, во многом применимы и к отсортированным строковым таблицам. Основное отличие здесь заключается в том, что ячейки записываются последовательно и не изменяются в течение жизненного цикла таблицы. Поскольку индексные файлы содержат указатели на записи данных, хранящиеся в файле данных, смещения данных должны быть известны к моменту создания индекса. Во время уплотнения файлы данных можно читать последовательно, не обращаясь к индексу, так как записи данных в них уже упорядочены. Поскольку таблицы, подвергаемые слиянию во время уплотнения, отсортированы в одинаковом порядке и итерация слиянием сохраняет порядок, результирующая объединенная таблица также создается путем последовательного занесения записей данных за один подход. Как только файл полностью записан, он считается неизменяемым и его находящееся на диске содержимое не подвергается модификации.

**Фильтры Блума**\
Источником увеличения объема чтения в LSM-деревьях является то, что для выполнения операции чтения мы должны обращаться к нескольким дисковым таблицам. Это происходит потому, что мы не всегда заранее знаем, содержит ли дисковая таблица запись данных для искомого ключа. Один из способов обойтись без поиска по таблице сводится к тому, чтобы хранить диапазон ее ключей (наименьший и наибольший ключи, хранящиеся в этой таблице) в метаданных и проверять, принадлежит ли искомый ключ диапазону этой таблицы. Эта информация неточна и сообщает только о том, может ли запись данных присутствовать в таблице. В качестве усовершенствования многие реализации, включая Apache Cassandra (https://databass.dev/links/78) и RocksDB (https://databass.dev/links/79), используют структуру данных, называемую фильтром Блума.
Фильтр Блума позволяет выяснить, что ключ может находиться в таблице или что он определенно не находится в таблице. Файлы, для которых фильтр Блума возвращает отрицательное совпадение, пропускаются во время выполнения запроса. Остальные файлы проверяются на наличие искомой записи данных. Применение фильтров Блума к таблицам на диске позволяет значительно сократить число таблиц, к которым осуществляется доступ во время чтения.
В фильтре Блума используются большой битовый массив и несколько хэш-функций. Хэш-функции применяются к ключам записей таблицы для поиска индексов, биты которых в битовом массиве установлены в 1. Биты, установленные в 1 во всех позициях, определенных хэш-функциями, указывают на присутствие ключа во множестве. В ходе поиска при проверке наличия элемента в фильтре Блума для ключа снова вычисляются хэш-функции, и если биты, определяемые всеми хэш-функциями, равны 1, мы возвращаем положительный результат, утверждающий, что элемент
является членом множества с определенной вероятностью. Если хотя бы один из битов равен 0, мы можем точно сказать, что элемент отсутствует во множестве.
Хэш-функции, применяемые к разным ключам, могут возвращать одну и ту же позицию бита и приводить к хэш-конфликту, а установленные в 1 биты означают только то, что некоторая хэш-функция выдала эту позицию бита для некоторого ключа. Для уменьшения вероятности конфликта можно использовать битовый массив большего размера, а для увеличения точности использовать больше хэш-функций. Увеличение размера массива ведет к накладным расходам по памяти, ну а использование большего количества хеш-функций может отрицательно повлиять на производительность. Поэтому необходимо найти баланс. 

**Список с пропусками**\
Существует множество различных структур данных для хранения отсортированных данных в памяти, и одна из них, которая в последнее время становится все более популярной благодаря своей простоте, называется список с пропусками. С точки зрения реализации список с пропусками не намного сложнее, чем односвязный список, и его вероятностные гарантии сложности близки к таковым у деревьев поиска.
Списки с пропусками не требуют вращения или перемещения для операций вставки и обновления, а вместо этого используют вероятностную балансировку. Списки с пропусками, как правило, менее эффективны при кэшировании, чем резидентные B-деревья, поскольку узлы списков малы и распределяются в памяти произвольным образом. Некоторые реализации повышают эффективность кэширования, используя развернутые связанные списки (https://databass.dev/links/80).
Список с пропусками состоит из ряда узлов разной высоты, выстраивающих связанные иерархии, позволяющие пропускать диапазоны элементов. Каждый узел содержит ключ, и, в отличие от узлов в связанном списке, некоторые узлы имеют более одного последующего элемента. Узел с высотой h связан с одним или несколькими предшествующими узлами с высотой не более h. Узлы на самом нижнем уровне могут быть связаны с узлами любой высоты.
Высота узла определяется случайной функцией и вычисляется во время вставки. Узлы с одинаковой высотой образуют уровень. Количество уровней ограничено, чтобы избежать бесконечного роста, а максимальная высота выбирается исходя из максимального количества элементов, вмещаемого структурой. На каждом следующем уровне узлов экспоненциально меньше.
Поисковые запросы выполняются путем перехода по указателям узлов на самом высоком уровне. Как только процесс поиска обнаруживает узел, содержащий ключ, который больше, чем искомый, совершается переход по ссылке предшественника на узел, находящийся на следующем уровне. То есть если искомый ключ больше, чем ключ текущего узла, поиск продолжается по направлению вперед. Если искомый ключ меньше ключа текущего узла, то поиск продолжается с предыдущего узла на следующем уровне. Этот процесс повторяется рекурсивно до тех пор, пока не будет найден искомый ключ или его предшественник.

**Доступ к диску**\
Поскольку большая часть содержимого таблиц находится на диске, а устройства хранения обычно позволяют осуществлять доступ к данным поблочно, многие реализации LSM-дерева используют кэш страниц для доступа к диску и промежуточного кэширования. 
Наиболее заметным отличием является то, что содержимое в памяти неизменяемо и поэтому не требует дополнительных блокировок или защелок для параллельного доступа. Подсчет ссылок применяется, чтобы не допустить вытеснения из памяти страниц, к которым в текущий момент осуществляется доступ, и чтобы текущие запросы были выполнены до удаления файлов, лежащих в основе структуры, во время уплотнения.
Другое отличие состоит в том, что записи данных в LSM-деревьях не обязательно выравнены по страницам, а указатели могут быть реализованы для адресации с использованием абсолютных смещений, а не идентификаторов страниц. На рис. 7.9 показаны записи с содержимым, не выровненным по дисковым блокам. Некоторые записи выходят за границы страниц и требуют загрузки в память нескольких страниц. 

**Сжатие**\
Чтобы иметь возможность адресовать сжатые страницы, нам нужно отслеживать границы адресов при записи их содержимого. Мы могли бы заполнять сжатые страницы нулями, выравнивая их по размеру страницы, но тогда не было бы пользы от сжатия.
Для обеспечения адресуемости сжатых страниц нам нужен уровень косвенности, сохраняющий смещения и размеры сжатых страниц. На рис. 7.10 показан процесс отображения сжатых блоков на несжатые блоки. Сжатые страницы всегда меньше исходных страниц, так как в противном случае нет смысла их сжимать.

<img width="731" alt="Capture d’écran 2024-10-31 à 12 01 59 PM" src="https://github.com/user-attachments/assets/6e936594-b871-48ca-878b-c4399898a63d">

Во время уплотнения и выгрузки на диск сжатые страницы добавляются последовательно, а информация о сжатии (исходное смещение несжатой страницы и фактическое смещение сжатой страницы) сохраняется в отдельном сегменте файла. Во время чтения ищутся смещение сжатой страницы и ее размер, после чего страница может быть распакована и размещена в памяти.

### Неупорядоченное LSM-хранилище

**Bitcask**\
Bitcask (https://databass.dev/links/82), одна из подсистем хранения, используемых в СУБД Riak (https://databass.dev/links/83), является неупорядоченным журналированным хранилищем [SHEEHY10b]. В отличие от рассмотренных выше реализаций журналированного хранилища, она не использует резидентные таблицы для буферизации и хранит записи данных непосредственно в файлах журналов. 
Чтобы сделать значения доступными для поиска, в Bitcask используется структура данных keydir, которая содержит ссылки на последние записи данных с соответствующими
ключами. Старые записи данных могут по-прежнему присутствовать на диске, но на них нет ссылок из keydir и они удаляются при сборке мусора в процессе уплотнения. Структура keydir реализуется как резидентная хэш-карта и воссоздается заново из файлов журнала во время запуска.
При выполнении операции записи ключ и запись данных последовательно добавляются в файл журнала, а в структуру keydir помещается указатель на позицию вновь сделанной записи данных.
Операции чтения просматривают структуру keydir, чтобы найти искомый ключ, и переходят по соответствующему указателю к файлу журнала, находя таким образом запись данных. Поскольку в любой момент у ключа может быть только одно связанное с ним значение в структуре keydir, точечные запросы не должны приводить к слиянию данных из нескольких источников.
На рис. 7.11 показан процесс отображения ключей на записи в файлах данных в подсистеме хранения Bitcask. Файлы журнала содержат записи данных, а структура keydir указывает на последнюю актуальную запись данных, связанную с каждым ключом. Затененные записи в файлах данных (замещаемые последующими операциями записи или удаления) выделены серым цветом.

<img width="778" alt="Capture d’écran 2024-10-31 à 12 10 59 PM" src="https://github.com/user-attachments/assets/e9269473-2304-4e91-a523-9f9525f6244f">

Во время уплотнения содержимое всех файлов журнала последовательно считывается, подвергается слиянию и записывается в новое место. Сохраняются только текущие записи данных, а затененные — отбрасываются. Структура keydir обновляется новыми указателями на перемещенные записи данных. Записи данных хранятся непосредственно в файлах журналов, поэтому вести отдельный журнал упреждающей записи не требуется, что уменьшает и объем занимаемого пространства, и показатель увеличения объема записи. Недостатком этого подхода является то, что он предлагает только точечные запросы и не позволяет сканировать диапазон, так как элементы неупорядочены и в структуре keydir, и в файлах данных.
Преимуществами такого подхода являются простота и высокая производительность точечных запросов. Несмотря на то что существует несколько версий записей данных, только последняя версия адресуется структурой keydir. Однако необходимость хранить все ключи в памяти и заново воссоздавать структуру keydir при запуске является ограничением, которое может стать решающим отрицательным фактором для некоторых сценариев использования. Хотя этот подход отлично подходит для точечных запросов, он не поддерживает запросы диапазона.

<img width="807" alt="Capture d’écran 2024-10-31 à 11 55 40 AM" src="https://github.com/user-attachments/assets/299a9a33-a0f0-4aa3-9893-835aa4687a45">

**WiscKey**\
Для многих сфер применения важны запросы диапазона, и было бы здорово иметь структуру хранения, которая обладала бы преимуществами по объему записи и использованию пространства, свойственными неупорядоченному хранению, в то же время позволяя нам выполнять сканирование диапазона. Хранилище WiscKey отделяет сортировку от сборки мусора за счет размещения ключей в отсортированном виде в LSM-деревьях с размещением записей данных в неупорядоченных файлах, доступных только для добавления, называемых «журналами значений» (vLog). Такой подход может убрать две проблемы, упомянутые при обсуждении подсистемы Bitcask: необходимость в сохранении всех ключей в памяти и в воссоздании хэш-таблицы при запуске.
На рис. 7.12 показаны ключевые компоненты хранилища WiscKey и процесс отображения ключей на файлы журнала. Файлы vLog содержат неупорядоченные записи данных. Ключи хранятся в отсортированных LSM-деревьях, указывая на последние записи данных в файлах журнала. Поскольку ключи обычно намного меньше, чем связанные с ними записи данных, их уплотнение гораздо эффективнее. Такой подход может быть особенно полезен в случаях с низкой частотой обновлений и удалений, когда сборка мусора освобождает не так уж много места на диске.
Основная проблема здесь заключается в том, что, поскольку данные файлов vLog неотсортированы, сканирование диапазона требует произвольного ввода-вывода. В хранилище WiscKey используется внутренний параллелизм твердотельных накопителей для параллельной предварительной выборки блоков во время сканиро вания диапазона и снижения затрат на произвольный ввод-вывод. С точки зрения поблочной передачи затраты все равно остаются высокими: чтобы извлечь одну запись данных во время сканирования диапазона, необходимо прочитать всю страницу, на
которой она находится.

<img width="793" alt="Capture d’écran 2024-10-31 à 12 25 13 PM" src="https://github.com/user-attachments/assets/62873755-c746-43a4-a7f1-6977f701bede">

Во время уплотнения содержимое файла vLog последовательно считывается, подвергается слиянию и записывается в новое место. Указатели (значения в LSM-дереве с ключами) обновляются таким образом, чтобы указывать на эти новые позиции. Чтобы избежать сканирования всего содержимого файла vLog, в хранилище WiscKey используются указатели на голову и хвост, содержащие информацию о сегментах файла vLog, которые содержат актуальные ключи. 
Поскольку данные в файле vLog неотсортированы и не содержат информации об актуальности, то, чтобы определить, какие значения все еще актуальны, необходимо просканировать дерево ключей. Выполнение этих проверок во время сборки мусора создает дополнительную сложность — традиционные LSM-деревья могут находить содержимое файла во время уплотнения без обращения к индексу с ключами.

<a name="part_1_conclusion"></a>
### Часть I. Заключение
В заключение первой части автор привел вот такую полезную схему 

<img width="787" alt="Capture d’écran 2024-10-28 à 5 50 32 PM" src="https://github.com/user-attachments/assets/938d6c4e-83cd-423c-842f-b64f0653def6">

### Bonus. Database Internals CMU course
В качестве бонуса представляю также заметки по CMU Database Course от Andy Pavlo. 
1. **Lecture #03: Database Storage I** https://15445.courses.cs.cmu.edu/fall2024/notes/03-storage1.pdf
2. **Lecture #04: Database Storage II** https://15445.courses.cs.cmu.edu/fall2024/notes/04-storage2.pdf
3. **Lecture #05: Storage Models & Compression** https://15445.courses.cs.cmu.edu/fall2024/notes/05-storage3.pdf
